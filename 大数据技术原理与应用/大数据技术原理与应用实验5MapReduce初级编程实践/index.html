<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大数据技术原理与应用实验5MapReduce初级编程实践 | hsuwindowBlogs</title><meta name="author" content="hsuwindow"><meta name="copyright" content="hsuwindow"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="实验5 MapReduce初级编程实践1.  实验目的（1） 通过实验掌握基本的 MapReduce 编程方法； （2） 掌握用 MapReduce 解决一些常见的数据处理问题，包括数据去重、数据排序和数据挖掘等。 2.实验平台（1） 操作系统： Linux（Ubuntu22.04）； （2） Hadoop 版本： 3.1.3；  3.实验步骤与结果（一）编程实现文件合并和去重操作 对于两个输入文">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据技术原理与应用实验5MapReduce初级编程实践">
<meta property="og:url" content="http://hsuwindow.vip/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C5MapReduce%E5%88%9D%E7%BA%A7%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="hsuwindowBlogs">
<meta property="og:description" content="实验5 MapReduce初级编程实践1.  实验目的（1） 通过实验掌握基本的 MapReduce 编程方法； （2） 掌握用 MapReduce 解决一些常见的数据处理问题，包括数据去重、数据排序和数据挖掘等。 2.实验平台（1） 操作系统： Linux（Ubuntu22.04）； （2） Hadoop 版本： 3.1.3；  3.实验步骤与结果（一）编程实现文件合并和去重操作 对于两个输入文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://hsuwindow.vip/img/ubuntu_icon.png">
<meta property="article:published_time" content="2023-04-20T07:59:02.000Z">
<meta property="article:modified_time" content="2023-04-20T08:13:00.241Z">
<meta property="article:author" content="hsuwindow">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://hsuwindow.vip/img/ubuntu_icon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://hsuwindow.vip/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C5MapReduce%E5%88%9D%E7%BA%A7%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        if (name && globalFn[key][name]) return
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据技术原理与应用实验5MapReduce初级编程实践',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-20 16:13:00'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/ubuntu_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">hsuwindowBlogs</span></a><a class="nav-page-title" href="/"><span class="site-name">大数据技术原理与应用实验5MapReduce初级编程实践</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大数据技术原理与应用实验5MapReduce初级编程实践</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-20T07:59:02.000Z" title="发表于 2023-04-20 15:59:02">2023-04-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-20T08:13:00.241Z" title="更新于 2023-04-20 16:13:00">2023-04-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/">大数据技术原理与应用</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="实验5-MapReduce初级编程实践"><a href="#实验5-MapReduce初级编程实践" class="headerlink" title="实验5 MapReduce初级编程实践"></a>实验5 MapReduce初级编程实践</h1><h1 id="1-实验目的"><a href="#1-实验目的" class="headerlink" title="1.  实验目的"></a>1.  实验目的</h1><p>（1） 通过实验掌握基本的 MapReduce 编程方法；</p>
<p>（2） 掌握用 MapReduce 解决一些常见的数据处理问题，包括数据去重、数据排序和数据挖掘等。</p>
<h1 id="2-实验平台"><a href="#2-实验平台" class="headerlink" title="2.实验平台"></a>2.实验平台</h1><p>（1） 操作系统： Linux（Ubuntu22.04）；</p>
<p>（2） Hadoop 版本： 3.1.3； </p>
<h1 id="3-实验步骤与结果"><a href="#3-实验步骤与结果" class="headerlink" title="3.实验步骤与结果"></a>3.实验步骤与结果</h1><p><strong>（一）编程实现文件合并和去重操作</strong></p>
<p>对于两个输入文件，即文件 A 和文件 B，请编写 MapReduce 程序，对两个文件进行合并，并剔除其中重复的内容，得到一个新的输出文件 C。下面是输入文件和输出文件的一个样例供参考。</p>
<p>输入文件 A 的样例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">20170101 x   </span><br><span class="line"></span><br><span class="line">20170102 y  </span><br><span class="line"></span><br><span class="line">20170103 x </span><br><span class="line"></span><br><span class="line">20170104 y </span><br><span class="line"></span><br><span class="line">20170105 z </span><br><span class="line"></span><br><span class="line">20170106 x  </span><br></pre></td></tr></table></figure>

<p>输入文件 B 的样例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">20170101 y </span><br><span class="line"></span><br><span class="line">20170102 y </span><br><span class="line"></span><br><span class="line">20170103 x </span><br><span class="line"></span><br><span class="line">20170104 z </span><br><span class="line"></span><br><span class="line">20170105 y  </span><br></pre></td></tr></table></figure>

<p>根据输入文件 A 和 B 合并得到的输出文件 C 的样例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">20170101 x  </span><br><span class="line"></span><br><span class="line">20170101 y </span><br><span class="line"></span><br><span class="line">20170102 y </span><br><span class="line"></span><br><span class="line">20170103 x </span><br><span class="line"></span><br><span class="line">20170104 y </span><br><span class="line"></span><br><span class="line">20170104 z </span><br><span class="line"></span><br><span class="line">20170105 y </span><br><span class="line"></span><br><span class="line">20170105 z </span><br><span class="line"></span><br><span class="line">20170106 x  </span><br></pre></td></tr></table></figure>

<p><strong>A.操作过程</strong></p>
<p><strong>1.启动 hadoop</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image002.png" alt="img"></p>
<p><strong>2.需要首先删除HDFS中与当前Linux用户hadoop对应的input和output目录，这样确保后面程序运行不会出现问题。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image004.png" alt="img"></p>
<p><strong>3.再在HDFS中新建与当前Linux用户hadoop对应的input目录。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image006.png" alt="img"></p>
<p><strong>4.创建A.txt,B.txt，输入上述内容</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image007.png" alt="img"></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image008.png" alt="img"></p>
<p><strong>5.将A，B上传到HDFS中</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image010.png" alt="img"></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image012.png" alt="img"></p>
<p><strong>B.实验代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Merge</span> &#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">* 对A,B两个文件进行合并，并剔除其中重复的内容，得到一个新的输出文件C</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//重载map函数，直接将输入中的value复制到输出数据的key上</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Map</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, Text&gt;&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line">text = value;</span><br><span class="line">context.write(text, <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//重载reduce函数，直接将输入中的key复制到输出数据的key上</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context )</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line">context.write(key, <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">conf.set(<span class="string">&quot;fs.default.name&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;input&quot;</span>,<span class="string">&quot;output&quot;</span>&#125;; <span class="comment">/* 直接设置输入参数 */</span></span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">System.err.println(<span class="string">&quot;Usage: wordcount &lt;in&gt;&lt;out&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf,<span class="string">&quot;Merge and duplicate removal&quot;</span>);</span><br><span class="line">job.setJarByClass(Merge.class);</span><br><span class="line">job.setMapperClass(Map.class);</span><br><span class="line">job.setCombinerClass(Reduce.class);</span><br><span class="line">job.setReducerClass(Reduce.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">1</span>]));</span><br><span class="line">System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>C.运行结果</strong></p>
<p><strong><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image018.png" alt="img"></strong></p>
<p><strong>（二）编写程序实现对输入文件的排序</strong></p>
<p>现在有多个输入文件，每个文件中的每行内容均为一个整数。要求读取所有文件中的整数，进行升序排序后，输出到一个新的文件中，输出的数据格式为每行两个整数，第一个数字为第二个整数的排序位次，第二个整数为原待排列的整数。下面是输入文件和输出文件的一个样例供参考。</p>
<p>输入文件 1 的样例如下：</p>
<p>  33   37   12   40  </p>
<p>输入文件 2 的样例如下：</p>
<p>  4   16   39   5  </p>
<p>输入文件 3 的样例如下：</p>
<p>  1   45   25  </p>
<p>根据输入文件 1、 2 和 3 得到的输出文件如下：</p>
<p>  1 1   2 4   3 5   4 12   5 16   6 25   7 33   8 37   9 39   10 40   11 45  </p>
<p><strong>A.操作过程</strong></p>
<p><strong>1.启动 hadoop</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image020.png" alt="img"></p>
<p><strong>2.需要首先删除HDFS中与当前Linux用户hadoop对应的input和output目录，这样确保后面程序运行不会出现问题。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image022.png" alt="img"></p>
<p><strong>3.再在HDFS中新建与当前Linux用户hadoop对应的input目录。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image024.png" alt="img"></p>
<p><strong>4.创建FileOne.txt,FileTwo.txt,FileThree.txt输入上述内容</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image026.png" alt="img"></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image028.png" alt="img"></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image030.png" alt="img"></p>
<p><strong>5.将FileOne.txt,FileTwo.txt,FileThree.txt上传到HDFS中</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image032.png" alt="img"></p>
<p><strong>B.实验代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">simple_data_mining</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">time</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">* 输入一个child-parent的表格</span></span><br><span class="line"><span class="comment">* 输出一个体现grandchild-grandparent关系的表格</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//Map将输入文件按照空格分割成child和parent，然后正序输出一次作为右表，反序输出一次作为左表，需要注意的是在输出的value中必须加上左右表区别标志</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Map</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, Text&gt;&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">child_name</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>();</span><br><span class="line"><span class="type">String</span> <span class="variable">parent_name</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>();</span><br><span class="line"><span class="type">String</span> <span class="variable">relation_type</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>();</span><br><span class="line"><span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(line.charAt(i) != <span class="string">&#x27; &#x27;</span>)&#123;</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line">String[] values = &#123;line.substring(<span class="number">0</span>,i),line.substring(i+<span class="number">1</span>)&#125;;</span><br><span class="line"><span class="keyword">if</span>(values[<span class="number">0</span>].compareTo(<span class="string">&quot;child&quot;</span>) != <span class="number">0</span>)&#123;</span><br><span class="line">child_name = values[<span class="number">0</span>];</span><br><span class="line">parent_name = values[<span class="number">1</span>];</span><br><span class="line">relation_type = <span class="string">&quot;1&quot;</span>;<span class="comment">//左右表区分标志</span></span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">1</span>]), <span class="keyword">new</span> <span class="title class_">Text</span>(relation_type+<span class="string">&quot;+&quot;</span>+child_name+<span class="string">&quot;+&quot;</span>+parent_name));</span><br><span class="line"><span class="comment">//左表</span></span><br><span class="line">relation_type = <span class="string">&quot;2&quot;</span>;</span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">0</span>]), <span class="keyword">new</span> <span class="title class_">Text</span>(relation_type+<span class="string">&quot;+&quot;</span>+child_name+<span class="string">&quot;+&quot;</span>+parent_name));</span><br><span class="line"><span class="comment">//右表</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt;&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values,Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line"><span class="keyword">if</span>(time == <span class="number">0</span>)&#123;   <span class="comment">//输出表头</span></span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;grand_child&quot;</span>), <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;grand_parent&quot;</span>));</span><br><span class="line">time++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="variable">grand_child_num</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">String grand_child[] = <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">10</span>];</span><br><span class="line"><span class="type">int</span> <span class="variable">grand_parent_num</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">String grand_parent[]= <span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">10</span>];</span><br><span class="line"><span class="type">Iterator</span> <span class="variable">ite</span> <span class="operator">=</span> values.iterator();</span><br><span class="line"><span class="keyword">while</span>(ite.hasNext())&#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">record</span> <span class="operator">=</span> ite.next().toString();</span><br><span class="line"><span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> record.length();</span><br><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">if</span>(len == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line"><span class="type">char</span> <span class="variable">relation_type</span> <span class="operator">=</span> record.charAt(<span class="number">0</span>);</span><br><span class="line"><span class="type">String</span> <span class="variable">child_name</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>();</span><br><span class="line"><span class="type">String</span> <span class="variable">parent_name</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>();</span><br><span class="line"><span class="comment">//获取value-list中value的child</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(record.charAt(i) != <span class="string">&#x27;+&#x27;</span>)&#123;</span><br><span class="line">child_name = child_name + record.charAt(i);</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line">i=i+<span class="number">1</span>;</span><br><span class="line"><span class="comment">//获取value-list中value的parent</span></span><br><span class="line"><span class="keyword">while</span>(i&lt;len)&#123;</span><br><span class="line">parent_name = parent_name+record.charAt(i);</span><br><span class="line">i++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//左表，取出child放入grand_child</span></span><br><span class="line"><span class="keyword">if</span>(relation_type == <span class="string">&#x27;1&#x27;</span>)&#123;</span><br><span class="line">grand_child[grand_child_num] = child_name;</span><br><span class="line">grand_child_num++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;<span class="comment">//右表，取出parent放入grand_parent</span></span><br><span class="line">grand_parent[grand_parent_num] = parent_name;</span><br><span class="line">grand_parent_num++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(grand_parent_num != <span class="number">0</span> &amp;&amp; grand_child_num != <span class="number">0</span> )&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> <span class="variable">m</span> <span class="operator">=</span> <span class="number">0</span>;m&lt;grand_child_num;m++)&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> n=<span class="number">0</span>;n&lt;grand_parent_num;n++)&#123;</span><br><span class="line">context.write(<span class="keyword">new</span> <span class="title class_">Text</span>(grand_child[m]), <span class="keyword">new</span> <span class="title class_">Text</span>(grand_parent[n]));</span><br><span class="line"><span class="comment">//输出结果</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">conf.set(<span class="string">&quot;fs.default.name&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;input&quot;</span>,<span class="string">&quot;output&quot;</span>&#125;; <span class="comment">/* 直接设置输入参数 */</span></span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">System.err.println(<span class="string">&quot;Usage: wordcount &lt;in&gt;&lt;out&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf,<span class="string">&quot;Single table join&quot;</span>);</span><br><span class="line">job.setJarByClass(simple_data_mining.class);</span><br><span class="line">job.setMapperClass(Map.class);</span><br><span class="line">job.setReducerClass(Reduce.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">1</span>]));</span><br><span class="line">System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>); </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<p><strong>C.运行结果</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image040.png" alt="img"></p>
<p><strong>（三）对给定的表格进行信息挖掘</strong></p>
<p>下面给出一个 child-parent 的表格，要求挖掘其中的父子辈关系，给出祖孙辈关系的表<br> 格。</p>
<p>输入文件内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">child parent </span><br><span class="line"></span><br><span class="line"> Steven Lucy </span><br><span class="line"></span><br><span class="line"> Steven Jack </span><br><span class="line"></span><br><span class="line"> Jone Lucy </span><br><span class="line"></span><br><span class="line"> Jone Jack </span><br><span class="line"></span><br><span class="line"> Lucy Mary </span><br><span class="line"></span><br><span class="line"> Lucy Frank </span><br><span class="line"></span><br><span class="line"> Jack Alice </span><br><span class="line"></span><br><span class="line"> Jack Jesse </span><br><span class="line"></span><br><span class="line"> David Alice </span><br><span class="line"></span><br><span class="line"> David Jesse </span><br><span class="line"></span><br><span class="line"> Philip David </span><br><span class="line"></span><br><span class="line"> Philip Alma </span><br><span class="line"></span><br><span class="line"> Mark David </span><br><span class="line"></span><br><span class="line"> Mark Alma  </span><br></pre></td></tr></table></figure>

<p>输出文件内容如下：</p>
<p>  grandchild  grandparent </p>
<p>  Steven Alice   Steven Jesse </p>
<p>  Jone Alice   Jone Jesse </p>
<p>  Steven Mary   Steven Frank </p>
<p>  Jone Mary   Jone Frank </p>
<p>  Philip Alice   Philip Jesse </p>
<p>  Mark Alice   Mark Jesse  </p>
<p><strong>A.操作过程</strong></p>
<p><strong>1.启动 hadoop</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image041.png" alt="img"></p>
<p><strong>2.需要首先删除HDFS中与当前Linux用户hadoop对应的input和output目录，这样确保后面程序运行不会出现问题。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image042.png" alt="img"></p>
<p><strong>3.再在HDFS中新建与当前Linux用户hadoop对应的input目录。</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image044.png" alt="img"></p>
<p><strong>4.创建childparent.txt输入上述内容</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image046.png" alt="img"></p>
<p><strong>5.将childparent.txt上传到HDFS中</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image048.png" alt="img"></p>
<p><strong>B.实验代码</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MergeSort</span> &#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">* 输入多个文件，每个文件中的每行内容均为一个整数</span></span><br><span class="line"><span class="comment">* 输出到一个新的文件中，输出的数据格式为每行两个整数，第一个数字为第二个整数的排序位次，第二个整数为原待排列的整数</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//map函数读取输入中的value，将其转化成IntWritable类型，最后作为输出key</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Map</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, IntWritable, IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line"><span class="type">String</span> <span class="variable">text</span> <span class="operator">=</span> value.toString();</span><br><span class="line">data.set(Integer.parseInt(text));</span><br><span class="line">context.write(data, <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//reduce函数将map输入的key复制到输出的value上，然后根据输入的value-list中元素的个数决定key的输出次数,定义一个全局变量line_num来代表key的位次</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;IntWritable, IntWritable, IntWritable, IntWritable&gt;&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">line_num</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(IntWritable key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException&#123;</span><br><span class="line"><span class="keyword">for</span>(IntWritable val : values)&#123;</span><br><span class="line">context.write(line_num, key);</span><br><span class="line">line_num = <span class="keyword">new</span> <span class="title class_">IntWritable</span>(line_num.get() + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义Partition函数，此函数根据输入数据的最大值和MapReduce框架中Partition的数量获取将输入数据按照大小分块的边界，然后根据输入数值和边界的关系返回对应的Partiton ID</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Partition</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;IntWritable, IntWritable&gt;&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(IntWritable key, IntWritable value, <span class="type">int</span> num_Partition)</span>&#123;</span><br><span class="line"><span class="type">int</span> <span class="variable">Maxnumber</span> <span class="operator">=</span> <span class="number">65223</span>;<span class="comment">//int型的最大数值</span></span><br><span class="line"><span class="type">int</span> <span class="variable">bound</span> <span class="operator">=</span> Maxnumber/num_Partition+<span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> <span class="variable">keynumber</span> <span class="operator">=</span> key.get();</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i&lt;num_Partition; i++)&#123;</span><br><span class="line"><span class="keyword">if</span>(keynumber&lt;bound * (i+<span class="number">1</span>) &amp;&amp; keynumber&gt;=bound * i)&#123;</span><br><span class="line"><span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line"><span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">conf.set(<span class="string">&quot;fs.default.name&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">String[] otherArgs = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;input&quot;</span>,<span class="string">&quot;output&quot;</span>&#125;; <span class="comment">/* 直接设置输入参数 */</span></span><br><span class="line"><span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">System.err.println(<span class="string">&quot;Usage: wordcount &lt;in&gt;&lt;out&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf,<span class="string">&quot;Merge and sort&quot;</span>);</span><br><span class="line">job.setJarByClass(MergeSort.class);</span><br><span class="line">job.setMapperClass(Map.class);</span><br><span class="line">job.setReducerClass(Reduce.class);</span><br><span class="line">job.setPartitionerClass(Partition.class);</span><br><span class="line">job.setOutputKeyClass(IntWritable.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">1</span>]));</span><br><span class="line">System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>C.运行结果</strong></p>
<p><img src="https://bradtorresblog.oss-cn-beijing.aliyuncs.com/BigDataExperiment5/clip_image058.png" alt="img"></p>
<h1 id="4-实验总结"><a href="#4-实验总结" class="headerlink" title="4.实验总结"></a>4.实验总结</h1><p><strong>（1）实验完成情况</strong></p>
<p>实验完成率：100%</p>
<p><strong>（2）出现的问题与解决方案</strong></p>
<p>问题1： 对MapReduce程序不熟悉</p>
<p>解决：参考<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2481-2/">http://dblab.xmu.edu.cn/blog/2481-2/</a> 的MapReduce编程实践教程</p>
<p>问题2： 启动hadoop时报错：util.NativeCodeLoader: Unable to load native-hadoop library for your platform</p>
<p>解决： 这个消息是一个警告，出现在Hadoop应用程序或服务的日志中，表示当前平台无法加载本机Hadoop库。Hadoop是使用Java语言开发的,但是有一些需求和操作并不适合使用java所以会引入了本地库（Native Libraries）的概念，通过本地库，Hadoop可以更加高效地执行某一些操作.</p>
<p><strong>方法一：</strong></p>
<p>在Hadoop的配置文件core-site.xml中可以设置是否使用本地库：（Hadoop默认的配置为启用本地库）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;name&gt;hadoop.<span class="keyword">native</span>.lib&lt;/name&gt;</span><br><span class="line"></span><br><span class="line"> &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line"></span><br><span class="line"> &lt;description&gt;Should <span class="keyword">native</span> hadoop libraries, <span class="keyword">if</span> present, be used.&lt;/description&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><strong>方法二：</strong></p>
<p>直接下载编译好的2.6.x-native-64位包，替换原来的native包</p>
<p>下载地址：<a target="_blank" rel="noopener" href="http://download.csdn.net/detail/u013310025/9657359">http://download.csdn.net/detail/u013310025/9657359</a></p>
<p>下载完后传到namenode 和datanode服务器上</p>
<p>删除native 下的所有包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /[hadoopHome的目录]/lib/native/*</span><br></pre></td></tr></table></figure>

<p>解压文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf hadoop-native-64-2.6.0.tar /[hadoopHome的目录]/lib/native</span><br></pre></td></tr></table></figure>

<p>再试着执行查看文件命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdoop fs -ls /</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://hsuwindow.vip">hsuwindow</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://hsuwindow.vip/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C5MapReduce%E5%88%9D%E7%BA%A7%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5/">http://hsuwindow.vip/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C5MapReduce%E5%88%9D%E7%BA%A7%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://hsuwindow.vip" target="_blank">hsuwindowBlogs</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a></div><div class="post-share"><div class="social-share" data-image="/img/ubuntu_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%AE%9E%E9%AA%8C%E5%AE%9E%E6%97%B6%E6%A3%80%E7%B4%A2/" title="大数据技术原理与应用华为云实验实时检索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">大数据技术原理与应用华为云实验实时检索</div></div><div class="info-2"><div class="info-item-1">1.实验介绍1.1.实验概述本实验基于华为云服务。通过模拟开发流程，包括数据导入库，组件应用开发，构建搜索服务，最终完成实时检索功能。 1.2.实验目的l 掌握大数据相关服务的购买及基础配置 l 掌握HBase应用开发的基本语法 l 掌握ElasticSearch应用开发的基本语法 l 掌握实时检索的功能实现 1.3.实验规划在同一VPC内的ECS通过内网访问MRS HBase和CSS各自的网络地址，并通过各自网络地址完成数据导入和数据查询。  1.4.实验思路（1）通过配置和申请华为云服务VPC，ECS，MRS和CSS作为基础配置。 （2）ECS在同一VPC内通过安全组规则访问MRS和CSS服务。 （3）在ECS上搭建基本应用开发环境。 （4）在ECS上建工程项目，开发基于MRS和CSS的应用程序。 （5）服务使用完毕，进行释放资源。 1.5.实验流程 2.实验平台与服务l ECS (Elastic Cloud...</div></div></div></a><a class="pagination-related" href="/Hexo/Hexo%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0Live2D%E5%B0%8F%E4%BA%BA/" title="Hexo博客添加Live2D小人"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hexo博客添加Live2D小人</div></div><div class="info-2"><div class="info-item-1">摘要通过hexo-helper-live2d插件给自己的博客添加一个小人。 小白入门：hexo 的官方是支持看板娘的，已经封装好了插件，但只是模型，不能说话、不能换装、功能较少。 大神水平：功能齐全。能说话、能换装、能玩游戏、能拍照、还能自定义。   Hexo博客添加Live2D小人通过hexo-helper-live2d插件给自己的博客添加一个小人，最后的成果图如下  小白入门首先安装插件1npm install --save hexo-helper-live2d  如果你的npm出现依赖问题 vulnerabilities...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%AE%9E%E9%AA%8C%E5%AE%9E%E6%97%B6%E5%88%86%E6%9E%90/" title="大数据技术原理与应用华为云实验实时分析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-30</div><div class="info-item-2">大数据技术原理与应用华为云实验实时分析</div></div><div class="info-2"><div class="info-item-1">1.实验目的l 掌握大数据相关服务的购买及基础配置 l 掌握使用Flume采集数据 l 掌握Flink SQL代码的编写 l 掌握使用DLV进行数据可视化 l 掌握实时流数据的处理流程 2.实验平台与服务l MRS (MapReduce Service) MapReduce服务是一个在华为云上部署和管理Hadoop系统的服务，一键即可部署Hadoop集群。 l DLI (Data Lake Insight) 数据湖探索是完全兼容Apache Spark、Apache Flink、openLooKeng（基于Apache Presto）生态，提供一站式的流处理、批处理、交互式分析的Serverless融合处理分析服务。 l RDS (Relational Database Service) 华为云关系型数据库是一种基于云计算平台的即开即用、稳定可靠、弹性伸缩、便捷管理的在线关系型数据库服务，支持单机和主备部署模式，支持MySQL、PostgreSQL、SQL Server等主流的关系型数据库引擎。 l CDM (Cloud Data...</div></div></div></a><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%AE%9E%E9%AA%8C%E5%AE%9E%E6%97%B6%E6%A3%80%E7%B4%A2/" title="大数据技术原理与应用华为云实验实时检索"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-25</div><div class="info-item-2">大数据技术原理与应用华为云实验实时检索</div></div><div class="info-2"><div class="info-item-1">1.实验介绍1.1.实验概述本实验基于华为云服务。通过模拟开发流程，包括数据导入库，组件应用开发，构建搜索服务，最终完成实时检索功能。 1.2.实验目的l 掌握大数据相关服务的购买及基础配置 l 掌握HBase应用开发的基本语法 l 掌握ElasticSearch应用开发的基本语法 l 掌握实时检索的功能实现 1.3.实验规划在同一VPC内的ECS通过内网访问MRS HBase和CSS各自的网络地址，并通过各自网络地址完成数据导入和数据查询。  1.4.实验思路（1）通过配置和申请华为云服务VPC，ECS，MRS和CSS作为基础配置。 （2）ECS在同一VPC内通过安全组规则访问MRS和CSS服务。 （3）在ECS上搭建基本应用开发环境。 （4）在ECS上建工程项目，开发基于MRS和CSS的应用程序。 （5）服务使用完毕，进行释放资源。 1.5.实验流程 2.实验平台与服务l ECS (Elastic Cloud...</div></div></div></a><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C1%E7%86%9F%E6%82%89%E5%B8%B8%E7%94%A8%E7%9A%84Linux%E6%93%8D%E4%BD%9C%E5%92%8CHadoop%E6%93%8D%E4%BD%9C/" title="大数据技术原理与应用实验1熟悉常用的Linux操作和Hadoop操作"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-05</div><div class="info-item-2">大数据技术原理与应用实验1熟悉常用的Linux操作和Hadoop操作</div></div><div class="info-2"><div class="info-item-1">实验1 熟悉常用的Linux操作和Hadoop操作1.实验目的Hadoop运行在Linux系统上，因此，需要学习实践一些常用的Linux命令。本实验旨在熟悉常用的Linux操作和Hadoop操作，为顺利开展后续其他实验奠定基础。 2.实验平台（1）操作系统： Ubuntu-22.04.2-desktop-amd64 （2）Hadoop版本：3.1.3 3.实验步骤与结果*1.熟悉常用的Linux*操作 1）cd命令：切换目录 （1）   切换到目录“&#x2F;usr&#x2F;local”  （2）   切换到当前目录的上一级目录  （3）  ...</div></div></div></a><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C2%E7%86%9F%E6%82%89%E5%B8%B8%E7%94%A8%E7%9A%84HDFS%E6%93%8D%E4%BD%9C/" title="大数据技术原理与应用实验2熟悉常用的HDFS操作"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-05</div><div class="info-item-2">大数据技术原理与应用实验2熟悉常用的HDFS操作</div></div><div class="info-2"><div class="info-item-1">实验2 熟悉常用的 HDFS 操作1.实验目的（1） 理解 HDFS 在 Hadoop 体系结构中的角色； （2） 熟练使用 HDFS 操作常用的 Shell 命令； （3） 熟悉 HDFS 操作常用的 Java API。  2.实验平台（1） 操作系统： Linux（Ubuntu-22.04.2-desktop-amd64）； （2） Hadoop 版本： 3.1.3； （3） JDK 版本： 1.8； （4） Java IDE： Eclipse。  3.实验步骤与结果（一）编程实现以下功能，并利用 Hadoop 提供的 Shell 命令完成相同任务： （1） 向 HDFS 中上传任意文本文件，如果指定的文件在 HDFS...</div></div></div></a><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C3%E7%86%9F%E6%82%89%E5%B8%B8%E7%94%A8%E7%9A%84HBase%E6%93%8D%E4%BD%9C/" title="大数据技术原理与应用实验3熟悉常用的HBase操作"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-05</div><div class="info-item-2">大数据技术原理与应用实验3熟悉常用的HBase操作</div></div><div class="info-2"><div class="info-item-1">实验3 熟悉常用的HBase操作1.  实验目的（1） 理解 HBase 在 Hadoop 体系结构中的角色； （2） 熟练使用 HBase 操作常用的 Shell 命令； （3） 熟悉 HBase 操作常用的 Java API。 2.实验平台（1） 操作系统： Linux（Ubuntu-22.04.2-desktop-amd64）； （2） Hadoop 版本： 3.1.3； （3） HBase版本： （4） JDK 版本： 1.8； （5） Java IDE： Eclipse。  3.实验步骤与结果（一）编程实现以下指定功能，并用 Hadoop 提供的 HBase Shell 命令完成相同任务：     （1） 列出 HBase 所有的表的相关信息，例如表名； 运行结果+Java代码：  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.io.IOException;import...</div></div></div></a><a class="pagination-related" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E5%AE%9E%E9%AA%8C4NoSQL%E5%92%8C%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%93%8D%E4%BD%9C%E6%AF%94%E8%BE%83/" title="大数据技术原理与应用实验4NoSQL和关系数据库的操作比较"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-05</div><div class="info-item-2">大数据技术原理与应用实验4NoSQL和关系数据库的操作比较</div></div><div class="info-2"><div class="info-item-1">实验4 NoSQL和关系数据库的操作比较1.  实验目的（1）理解四种数据库(MySQL、 HBase、 Redis 和 MongoDB)的概念以及不同点； （2）熟练使用四种数据库操作常用的 Shell 命令； （3）熟悉四种数据库操作常用的 Java API。 2.实验平台（1） 操作系统： Linux（Ubuntu-22.04.2）； （2） Hadoop 版本： 3.1.3； （3） MySQL 版本： 8.0.32； （4） HBase 版本： 2.2.2； （5） Redis 版本： 7.0.10； （6） MongoDB 版本： 6.0.5； （7） JDK 版本： 1.8； （8） Java IDE： Eclipse； 3.实验步骤与结果（一） MySQL 数据库操作 学生表如 14-7 所示。 表 14-7 学生表 Student    Name English Math Computer    zhangsan 69 86 77   lisi 55 100 88   \1. 根据上面给出的 Student 表，在 MySQL 数据库中完成如下操作： （1）在...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/ubuntu_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">hsuwindow</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">84</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">24</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎光临本站,这是我日常工作和学习整理的总结,希望对你有所帮助.本站内容经过个人加工总结而来,也参考了网友们分享的资料,如有侵权,请第一时间联系我,我将及时进行修改或删除.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C5-MapReduce%E5%88%9D%E7%BA%A7%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5"><span class="toc-text">实验5 MapReduce初级编程实践</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84"><span class="toc-text">1.  实验目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%AE%9E%E9%AA%8C%E5%B9%B3%E5%8F%B0"><span class="toc-text">2.实验平台</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E6%AD%A5%E9%AA%A4%E4%B8%8E%E7%BB%93%E6%9E%9C"><span class="toc-text">3.实验步骤与结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C%E6%80%BB%E7%BB%93"><span class="toc-text">4.实验总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/%E8%AE%A4%E8%AF%86%20parquet%20%E6%96%87%E4%BB%B6(%E4%B8%80)/" title="认识 parquet 文件(一)">认识 parquet 文件(一)</a><time datetime="2025-08-04T13:24:20.000Z" title="发表于 2025-08-04 21:24:20">2025-08-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/uncategorized/%E8%AE%A4%E8%AF%86%20parquet%20%E6%96%87%E4%BB%B6(%E4%BA%8C)/" title="认识 parquet 文件(二)">认识 parquet 文件(二)</a><time datetime="2025-08-04T13:24:20.000Z" title="发表于 2025-08-04 21:24:20">2025-08-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E9%85%8D%E7%BD%AE/conda%20%E7%8E%AF%E5%A2%83%E5%92%8C%20Terminal%20%E6%98%BE%E7%A4%BA/" title="配置 Conda 环境与 Terminal 提示符 以及 conda 官方关于 prompt 刷新的说明和讨论">配置 Conda 环境与 Terminal 提示符 以及 conda 官方关于 prompt 刷新的说明和讨论</a><time datetime="2025-08-04T12:56:21.000Z" title="发表于 2025-08-04 20:56:21">2025-08-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%AE%97%E6%B3%95%E4%B8%8E%E5%AE%9E%E7%8E%B0/leetcode%E5%8F%8C%E6%8C%87%E9%92%88%E9%A2%98%E8%A7%A3/" title="leetcode双指针题解">leetcode双指针题解</a><time datetime="2025-06-30T15:38:21.000Z" title="发表于 2025-06-30 23:38:21">2025-06-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E7%AE%97%E6%B3%95%E4%B8%8E%E5%AE%9E%E7%8E%B0/leetcode%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%98%E8%A7%A3/" title="leetcode动态规划题解">leetcode动态规划题解</a><time datetime="2025-06-30T15:08:20.000Z" title="发表于 2025-06-30 23:08:20">2025-06-30</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By hsuwindow</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>